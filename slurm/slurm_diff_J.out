WARNING:tensorflow:From /home/hh2263/miniconda3/envs/drum-1.15/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2020-03-18 18:17:03.303050: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-03-18 18:17:04.099441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:3b:00.0
2020-03-18 18:17:06.401410: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-03-18 18:17:07.378469: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-03-18 18:17:10.456847: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-03-18 18:17:11.362186: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-03-18 18:17:12.404020: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-03-18 18:17:13.228035: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-03-18 18:17:26.973247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-18 18:17:26.975414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-03-18 18:17:26.975852: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-03-18 18:17:27.032327: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2020-03-18 18:17:27.032523: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557658a2d890 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-03-18 18:17:27.032545: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-03-18 18:17:27.034322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:3b:00.0
2020-03-18 18:17:27.034379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-03-18 18:17:27.034393: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-03-18 18:17:27.034406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-03-18 18:17:27.034418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-03-18 18:17:27.034430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-03-18 18:17:27.034443: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-03-18 18:17:27.034456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-18 18:17:27.036386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-03-18 18:17:27.062663: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-03-18 18:17:27.195318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-18 18:17:27.195355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-03-18 18:17:27.195366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-03-18 18:17:27.198850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15022 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)
2020-03-18 18:17:27.200671: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55765950cbf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-03-18 18:17:27.200689: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-16GB, Compute Capability 7.0
2020-03-18 18:18:06.493284: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-03-18 18:18:08.302032: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-18 18:18:13.678675: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
Train on 3200 samples, validate on 7776 samples
3200/3200 - 142s - loss: 0.3695 - mean_squared_error: 0.3695 - val_loss: 0.3383 - val_mean_squared_error: 0.3383
   32/10000 [..............................] - ETA: 0s - loss: 0.4565 - mean_squared_error: 0.4565  928/10000 [=>............................] - ETA: 0s - loss: 0.4094 - mean_squared_error: 0.4094 1824/10000 [====>.........................] - ETA: 0s - loss: 0.4028 - mean_squared_error: 0.4028 2720/10000 [=======>......................] - ETA: 0s - loss: 0.4071 - mean_squared_error: 0.4071 3616/10000 [=========>....................] - ETA: 0s - loss: 0.4065 - mean_squared_error: 0.4065 4512/10000 [============>.................] - ETA: 0s - loss: 0.4041 - mean_squared_error: 0.4041 5408/10000 [===============>..............] - ETA: 0s - loss: 0.4054 - mean_squared_error: 0.4054 6304/10000 [=================>............] - ETA: 0s - loss: 0.4039 - mean_squared_error: 0.4038 7200/10000 [====================>.........] - ETA: 0s - loss: 0.4058 - mean_squared_error: 0.4058 8096/10000 [=======================>......] - ETA: 0s - loss: 0.4059 - mean_squared_error: 0.4059 8992/10000 [=========================>....] - ETA: 0s - loss: 0.4073 - mean_squared_error: 0.4073 9888/10000 [============================>.] - ETA: 0s - loss: 0.4069 - mean_squared_error: 0.406910000/10000 [==============================] - 38s 4ms/sample - loss: 0.4068 - mean_squared_error: 0.4068
Traceback (most recent call last):
  File "/home/hh2263/miniconda3/envs/drum-1.15/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py", line 111, in save_model_to_hdf5
    f.flush()
  File "/home/hh2263/miniconda3/envs/drum-1.15/lib/python3.7/site-packages/h5py/_hl/files.py", line 452, in flush
    h5f.flush(self.id)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5f.pyx", line 146, in h5py.h5f.flush
RuntimeError: Unable to flush file's cached information (file write failed: time = Wed Mar 18 18:21:06 2020
, filename = '../output/tests/J-06_Q-01_order1.h5', file descriptor = 33, errno = 5, error message = 'Input/output error', buf = 0x55768127b160, total write size = 2528, bytes this sub-write = 2528, bytes actually written = 18446744073709551615, offset = 34608)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/hh2263/wave2shape/src/06_train.py", line 175, in <module>
    model_adjustable.save(epoch_network_path)
  File "/home/hh2263/miniconda3/envs/drum-1.15/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py", line 1171, in save
    signatures)
  File "/home/hh2263/miniconda3/envs/drum-1.15/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py", line 109, in save_model
    model, filepath, overwrite, include_optimizer)
  File "/home/hh2263/miniconda3/envs/drum-1.15/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py", line 114, in save_model_to_hdf5
    f.close()
  File "/home/hh2263/miniconda3/envs/drum-1.15/lib/python3.7/site-packages/h5py/_hl/files.py", line 443, in close
    h5i.dec_ref(id_)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5i.pyx", line 150, in h5py.h5i.dec_ref
RuntimeError: Can't decrement id ref count (file write failed: time = Wed Mar 18 18:21:06 2020
, filename = '../output/tests/J-06_Q-01_order1.h5', file descriptor = 33, errno = 5, error message = 'Input/output error', buf = 0x55765a53bb50, total write size = 6632, bytes this sub-write = 6632, bytes actually written = 18446744073709551615, offset = 112784)
/opt/slurm/data/slurmd/job8392638/slurm_script: line 19: 43423 Segmentation fault      (core dumped) python /home/hh2263/wave2shape/src/06_train.py 6 1
slurmstepd: error: *** JOB 8392638 ON gpu-90 CANCELLED AT 2020-03-18T18:33:56 ***
