{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "from tensorflow import keras\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv1D, AveragePooling1D, Conv2D, MaxPooling2D,ReLU\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import load_model #save and load models\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint\n",
    "import IPython.display as ipd\n",
    "from kymatio import Scattering1D\n",
    "import hitdifferentparts\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pescador\n",
    "import random\n",
    "import os\n",
    "import librosa\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "J = 8\n",
    "Q = 1\n",
    "order = 2\n",
    "\n",
    "#scattering_J-08_Q-01_order2_fold-val.pkl\n",
    "pkl_dir = '/scratch/hh2263/drum_data/han2020fa_sc-pkl/'\n",
    "#J = 8\n",
    "#Q = 1\n",
    "#order = 2\n",
    "pickle_name = \"_\".join(\n",
    "    [\"scattering\",\n",
    "     \"J-\" + str(J).zfill(2), \"Q-\" + str(Q).zfill(2), \"order\" + str(order)]\n",
    ")\n",
    "\n",
    "pkl_path_train = os.path.join(pkl_dir,pickle_name+\"_fold-train.pkl\")\n",
    "pkl_train = open(pkl_path_train, 'rb')\n",
    "Sy_train,y_train = pickle.load(pkl_train) \n",
    "\n",
    "pkl_path_val = os.path.join(pkl_dir,pickle_name+\"_fold-val.pkl\")\n",
    "pkl_val = open(pkl_path_val,'rb')\n",
    "Sy_val,y_val = pickle.load(pkl_val)\n",
    "\n",
    "pkl_path_test = os.path.join(pkl_dir,pickle_name+\"_fold-test.pkl\")\n",
    "pkl_test = open(pkl_path_test,'rb')\n",
    "Sy_test,y_test = pickle.load(pkl_test)\n",
    "\n",
    "#log scale p and D\n",
    "for idx in range(2,4):\n",
    "    y_train[:,idx] = [math.log10(i) for i in y_train[:,idx]]\n",
    "    y_test[:,idx] = [math.log10(i) for i in y_test[:,idx]]\n",
    "    y_val[:,idx] = [math.log10(i) for i in y_val[:,idx]]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(y_train)\n",
    "y_train_normalized = scaler.transform(y_train)\n",
    "y_val_normalized = scaler.transform(y_val)\n",
    "y_test_normalized = scaler.transform(y_test)\n",
    "\n",
    "#log scale the input\n",
    "eps = 1e-11\n",
    "Sy_train_log2 = np.log1p(((Sy_train>0)*Sy_train)/eps)\n",
    "Sy_val_log2 = np.log1p(((Sy_val>0)*Sy_val)/eps)\n",
    "Sy_test_log2 = np.log1p((Sy_test>0)*Sy_test/eps)\n",
    "\n",
    "#train the model\n",
    "trial_dir = \"../output/tests/\"\n",
    "os.makedirs(trial_dir, exist_ok=True)\n",
    "best_validation_loss = np.inf\n",
    "zoom_factor = 1\n",
    "n = Sy_train.shape[0]\n",
    "shape_time = round(Sy_train.shape[1] * zoom_factor)\n",
    "steps_per_epoch = 50\n",
    "bs = 64\n",
    "m = bs*steps_per_epoch\n",
    "idx = np.arange(0,n,1)\n",
    "val_loss=[]\n",
    "train_loss = []\n",
    "\n",
    "def create_model_adjustable(J,Q,order,k_size,nchan_out,zoom_factor,activation):\n",
    "    N = 2**15\n",
    "    y = np.random.rand(N)\n",
    "    scattering = Scattering1D(J = J,shape=(N,), Q = Q, max_order=order)\n",
    "    Sy = np.array(scattering(torch.Tensor(y))).T\n",
    "    input_x,input_y = Sy.shape\n",
    "    nchan_in = 1       # number of input channels.  1 since it is BW\n",
    "  \n",
    "    #adjustable input dimension!!!!\n",
    "    if zoom_factor == 0.5:\n",
    "        layer_size = 3\n",
    "    elif zoom_factor == 0.25:\n",
    "        layer_size = 2\n",
    "    elif zoom_factor == 1:\n",
    "        layer_size = 4\n",
    "        \n",
    "    zoomed_x = round(input_x * zoom_factor)\n",
    "    \n",
    "    \n",
    "    input_shape = (zoomed_x,input_y)#Sy.shape\n",
    "    kernel_size = (k_size,)\n",
    "    K.clear_session()\n",
    "    model=Sequential()\n",
    "    #1 conv layer +  1 batch normalization + nonlinear activation + pooling\n",
    "    model.add(BatchNormalization(input_shape=input_shape))\n",
    "    model.add(Conv1D(filters=nchan_out,\n",
    "                     kernel_size=kernel_size, padding=\"same\",name='conv1'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(AveragePooling1D(pool_size=(4,)))\n",
    "    \n",
    "   #second time\n",
    "    model.add(Conv1D(filters=nchan_out,\n",
    "                     kernel_size=kernel_size, padding=\"same\",name='conv2' ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(AveragePooling1D(pool_size=(4,)))\n",
    "    \n",
    "    #third time\n",
    "    if layer_size>=3:\n",
    "        model.add(Conv1D(filters=nchan_out,\n",
    "                         kernel_size=kernel_size, padding=\"same\",name='conv3' ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(AveragePooling1D(pool_size=(4,)))\n",
    "        if layer_size==4:\n",
    "        #fourth time\n",
    "            model.add(Conv1D(filters=nchan_out,\n",
    "                             kernel_size=kernel_size, padding=\"same\",name='conv4' ))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Activation(\"relu\"))\n",
    "            model.add(AveragePooling1D(pool_size=(2,)))\n",
    "            if layer_size ==5:\n",
    "                model.add(Conv1D(filters=nchan_out,\n",
    "                             kernel_size=kernel_size, padding=\"same\",name='conv5' ))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Activation(\"relu\"))\n",
    "                model.add(AveragePooling1D(pool_size=(2,)))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #what activation should be chosen for last layer, for regression problem? should be a linear function\n",
    "    model.add(Dense(5, activation=activation)) #output layer that corresponds to the 5 physical parameters.\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n",
    "model_adjustable = create_model_adjustable(J=J,Q=Q,order=order,k_size=8,nchan_out=16,zoom_factor=zoom_factor,activation='linear')\n",
    "#model_adjustable.summary()\n",
    "for epoch in range(30):\n",
    "    np.random.shuffle(idx)\n",
    "    Sy_temp = Sy_train_log2[idx[:m],:shape_time,:]\n",
    "    y_temp = y_train_normalized[idx[:m],:]\n",
    "    \n",
    "    hist = model_adjustable.fit(Sy_temp,\n",
    "                y_temp,\n",
    "                epochs=1,\n",
    "                verbose=2,\n",
    "                batch_size=bs,\n",
    "                validation_data = (Sy_val_log2[:,-shape_time:,:],y_val_normalized),\n",
    "                use_multiprocessing=False)\n",
    "    validation_loss = hist.history['val_loss'][0]\n",
    "    val_loss.append(validation_loss)\n",
    "    train_loss.append(hist.history['loss'][0])\n",
    "#    if validation_loss < best_validation_loss:\n",
    "#        best_validation_loss = validation_loss\n",
    "#        #epoch_str = \"epoch-\" + str(epoch).zfill(3)\n",
    "#        epoch_network_path = os.path.join(\n",
    "#           trial_dir, \"_\".join([ \"J-\" + str(J).zfill(2), \"Q-\" + str(Q).zfill(2), \"order\" + str(order)]) + \".h5\")\n",
    "#        model.save(epoch_network_path)\n",
    "        \n",
    "\n",
    "\n",
    "plt.plot(val_loss)\n",
    "plt.plot(train_loss)\n",
    "plt.legend(['validation loss','training loss'])\n",
    "plt.title('64 batch size, 50 steps per epoch,conv+batch')\n",
    "print(min(val_loss))\n",
    "plt.show()\n",
    "\n",
    "val_loss2 = []\n",
    "train_loss2 = []\n",
    "model_adjustable2 = create_model_adjustable(J=J,Q=Q,order=order,k_size=8,nchan_out=16,zoom_factor=zoom_factor,activation='linear')\n",
    "#model_adjustable.summary()\n",
    "for epoch in range(30):\n",
    "    np.random.shuffle(idx)\n",
    "    Sy_temp = Sy_train_log2[idx[:m],:shape_time,:]\n",
    "    y_temp = y_train_normalized[idx[:m],:]\n",
    "    \n",
    "    hist = model_adjustable2.fit(Sy_temp,\n",
    "                y_temp,\n",
    "                epochs=1,\n",
    "                verbose=2,\n",
    "                batch_size=bs,\n",
    "                validation_data = (Sy_val_log2[:,-shape_time:,:],y_val_normalized),\n",
    "                use_multiprocessing=False)\n",
    "    validation_loss = hist.history['val_loss'][0]\n",
    "    val_loss2.append(validation_loss)\n",
    "    train_loss2.append(hist.history['loss'][0])\n",
    "\n",
    "plt.plot(val_loss2)\n",
    "plt.plot(train_loss2)\n",
    "plt.legend(['validation loss','training loss'])\n",
    "plt.title('64 batch size, 50 steps per epoch,batch+conv')\n",
    "print(min(val_loss2))\n",
    "plt.show()\n",
    "\n",
    "model_adjustable3.summary()\n",
    "\n",
    "#zoom factor can only be 1/4, 1/2\n",
    "def create_model_adjustable(J,Q,order,k_size,nchan_out,activation):\n",
    "    N = 2**15\n",
    "    y = np.random.rand(N)\n",
    "    scattering = Scattering1D(J = J,shape=(N,), Q = Q, max_order=order)\n",
    "    Sy = np.array(scattering(torch.Tensor(y))).T\n",
    "    input_x,input_y = Sy.shape\n",
    "    nchan_in = 1       # number of input channels.  1 since it is BW\n",
    "  \n",
    "    input_shape = (input_x,input_y)#Sy.shape\n",
    "    kernel_size = (k_size,)\n",
    "    K.clear_session()\n",
    "    model=Sequential()\n",
    "    #1 conv layer +  1 batch normalization + nonlinear activation + pooling\n",
    "    #model.add(BatchNormalization(input_shape=input_shape))\n",
    "    model.add(Conv1D(input_shape=input_shape,filters=nchan_out,\n",
    "                     kernel_size=kernel_size, padding=\"same\",name='conv1'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    if model.layers[-1].output_shape[1]>=4:\n",
    "        pool = 4\n",
    "    elif model.layers[-1].output_shape[1]==2:\n",
    "        pool = 2\n",
    "\n",
    "    model.add(AveragePooling1D(pool_size=(pool,)))\n",
    "\n",
    "\n",
    "    for i in range(3):\n",
    "        model.add(Conv1D(filters=nchan_out,\n",
    "                     kernel_size=kernel_size, padding=\"same\" ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(\"relu\"))\n",
    "        #print('before pool',model.layers[-1].output_shape)\n",
    "        if model.layers[-1].output_shape[1] >= 4:\n",
    "            model.add(AveragePooling1D(pool_size=(4,)))\n",
    "        elif model.layers[-1].output_shape[1] == 2:\n",
    "            model.add(AveragePooling1D(pool_size=(2,)))\n",
    "        #print(model.layers[-1].output_shape)\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #what activation should be chosen for last layer, for regression problem? should be a linear function\n",
    "    model.add(Dense(5, activation='linear')) #output layer that corresponds to the 5 physical parameters.\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "print(model.layers[0].input_shape)\n",
    "\n",
    "## Complete rundown\n",
    "\n",
    "J=14\n",
    "Q=1\n",
    "order=2\n",
    "\n",
    "pkl_dir = '/scratch/hh2263/drum_data/han2020fa_sc-pkl/'\n",
    "#J = 8\n",
    "#Q = 1\n",
    "#order = 2\n",
    "pickle_name = \"_\".join(\n",
    "    [\"scattering\",\n",
    "     \"J-\" + str(J).zfill(2), \"Q-\" + str(Q).zfill(2), \"order\" + str(order)]\n",
    ")\n",
    "\n",
    "pkl_path_train = os.path.join(pkl_dir,pickle_name+\"_fold-train.pkl\")\n",
    "pkl_train = open(pkl_path_train, 'rb')\n",
    "Sy_train,y_train = pickle.load(pkl_train) \n",
    "\n",
    "pkl_path_val = os.path.join(pkl_dir,pickle_name+\"_fold-val.pkl\")\n",
    "pkl_val = open(pkl_path_val,'rb')\n",
    "Sy_val,y_val = pickle.load(pkl_val)\n",
    "\n",
    "pkl_path_test = os.path.join(pkl_dir,pickle_name+\"_fold-test.pkl\")\n",
    "pkl_test = open(pkl_path_test,'rb')\n",
    "Sy_test,y_test = pickle.load(pkl_test)\n",
    "\n",
    "#log scale p and D\n",
    "for idx in range(2,4):\n",
    "    y_train[:,idx] = [math.log10(i) for i in y_train[:,idx]]\n",
    "    y_test[:,idx] = [math.log10(i) for i in y_test[:,idx]]\n",
    "    y_val[:,idx] = [math.log10(i) for i in y_val[:,idx]]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(y_train)\n",
    "y_train_normalized = scaler.transform(y_train)\n",
    "y_val_normalized = scaler.transform(y_val)\n",
    "y_test_normalized = scaler.transform(y_test)\n",
    "\n",
    "#log scale the input\n",
    "eps = 1e-11\n",
    "Sy_train_log2 = np.log1p(((Sy_train>0)*Sy_train)/eps)\n",
    "Sy_val_log2 = np.log1p(((Sy_val>0)*Sy_val)/eps)\n",
    "Sy_test_log2 = np.log1p((Sy_test>0)*Sy_test/eps)\n",
    "\n",
    "trial_dir = \"../output/tests/\"\n",
    "os.makedirs(trial_dir, exist_ok=True)\n",
    "best_validation_loss = np.inf\n",
    "zoom_factor = 1\n",
    "n = Sy_train.shape[0]\n",
    "shape_time = round(Sy_train.shape[1] * zoom_factor)\n",
    "steps_per_epoch = 50\n",
    "bs = 64\n",
    "m = bs*steps_per_epoch\n",
    "idx = np.arange(0,n,1)\n",
    "val_loss=[]\n",
    "train_loss = []\n",
    "model_adjustable = create_model_adjustable(J=J,Q=Q,order=order,k_size=8,nchan_out=16,activation='linear')\n",
    "#model_adjustable.summary()\n",
    "for epoch in range(30):\n",
    "    np.random.shuffle(idx)\n",
    "    Sy_temp = Sy_train_log2[idx[:m],:shape_time,:]\n",
    "    y_temp = y_train_normalized[idx[:m],:]\n",
    "    \n",
    "    hist = model_adjustable.fit(Sy_temp,\n",
    "                y_temp,\n",
    "                epochs=1,\n",
    "                verbose=2,\n",
    "                batch_size=bs,\n",
    "                validation_data = (Sy_val_log2[:,-shape_time:,:],y_val_normalized),\n",
    "                use_multiprocessing=False)\n",
    "    validation_loss = hist.history['val_loss'][0]\n",
    "    val_loss.append(validation_loss)\n",
    "    train_loss.append(hist.history['loss'][0])\n",
    "   # if validation_loss < best_validation_loss:\n",
    "   #     best_validation_loss = validation_loss\n",
    "   #     #epoch_str = \"epoch-\" + str(epoch).zfill(3)\n",
    "   #     epoch_network_path = os.path.join(\n",
    "   #        trial_dir, \"_\".join([ \"J-\" + str(J).zfill(2), \"Q-\" + str(Q).zfill(2), \"order\" + str(order)]) + \".h5\")\n",
    "   #     model.save(epoch_network_path)\n",
    "        \n",
    "\n",
    "plt.plot(val_loss)\n",
    "plt.plot(train_loss)\n",
    "plt.legend(['validation loss','training loss'])\n",
    "plt.title('J=14,order=2,batch+norm')\n",
    "print(min(val_loss))\n",
    "plt.show()\n",
    "\n",
    "trial_dir = \"../output/tests/\"\n",
    "os.makedirs(trial_dir, exist_ok=True)\n",
    "best_validation_loss = np.inf\n",
    "zoom_factor = 1\n",
    "n = Sy_train.shape[0]\n",
    "shape_time = round(Sy_train.shape[1] * zoom_factor)\n",
    "steps_per_epoch = 50\n",
    "bs = 64\n",
    "m = bs*steps_per_epoch\n",
    "idx = np.arange(0,n,1)\n",
    "val_loss2=[]\n",
    "train_loss2 = []\n",
    "model_adjustable2 = create_model_adjustable(J=J,Q=Q,order=order,k_size=8,nchan_out=16,activation='linear')\n",
    "#model_adjustable.summary()\n",
    "for epoch in range(30):\n",
    "    np.random.shuffle(idx)\n",
    "    Sy_temp = Sy_train_log2[idx[:m],:shape_time,:]\n",
    "    y_temp = y_train_normalized[idx[:m],:]\n",
    "    \n",
    "    hist = model_adjustable2.fit(Sy_temp,\n",
    "                y_temp,\n",
    "                epochs=1,\n",
    "                verbose=2,\n",
    "                batch_size=bs,\n",
    "                validation_data = (Sy_val_log2[:,-shape_time:,:],y_val_normalized),\n",
    "                use_multiprocessing=False)\n",
    "    validation_loss = hist.history['val_loss'][0]\n",
    "    val_loss2.append(validation_loss)\n",
    "    train_loss2.append(hist.history['loss'][0])\n",
    "   # if validation_loss < best_validation_loss:\n",
    "   #     best_validation_loss = validation_loss\n",
    "   #     #epoch_str = \"epoch-\" + str(epoch).zfill(3)\n",
    "   #     epoch_network_path = os.path.join(\n",
    "   #        trial_dir, \"_\".join([ \"J-\" + str(J).zfill(2), \"Q-\" + str(Q).zfill(2), \"order\" + str(order)]) + \".h5\")\n",
    "   #     model.save(epoch_network_path)\n",
    "        \n",
    "\n",
    "plt.plot(val_loss2)\n",
    "plt.plot(train_loss2)\n",
    "plt.legend(['validation loss','training loss'])\n",
    "plt.title('J=14,order=2,conv + batchnorm')\n",
    "print(min(val_loss2))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
