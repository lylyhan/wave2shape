{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FiLM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "from tensorflow import keras\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv1D, AveragePooling1D, Conv2D, MaxPooling2D,ReLU\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import load_model #save and load models\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint\n",
    "import IPython.display as ipd\n",
    "from kymatio import Scattering1D\n",
    "import hitdifferentparts\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pescador\n",
    "import random\n",
    "import os\n",
    "import librosa\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download an example file from hpc\n",
    "pkl_trainpath = \"./scattering_fold-train_J-08_Q-01_order2.pkl\"\n",
    "pkl_valpath = \"./scattering_fold-val_J-08_Q-01_order2.pkl\"\n",
    "#pkl_testpath = \"./scattering_fold-test_J-08_Q-02_order2.pkl\"\n",
    "pkl_train = open(pkl_trainpath, 'rb')\n",
    "#pkl_test = open(pkl_testpath, 'rb')\n",
    "pkl_val = open(pkl_valpath, 'rb')\n",
    "\n",
    "Sy_train,y_train = pickle.load(pkl_train) \n",
    "#Sy_test,y_test = pickle.load(pkl_test) \n",
    "Sy_val,y_val = pickle.load(pkl_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps =1e-11\n",
    "Sy_train_log2 = np.log1p(((Sy_train>0)*Sy_train)/eps)\n",
    "Sy_val_log2 = np.log1p(((Sy_val>0)*Sy_val)/eps)\n",
    "#Sy_test_log2 = np.log1p((Sy_test>0)*Sy_test/eps)\n",
    "\n",
    "#log scale p and D\n",
    "for idx in range(2,4):\n",
    "    y_train[:,idx] = [math.log10(i) for i in y_train[:,idx]]\n",
    "    #y_test[:,idx] = [math.log10(i) for i in y_test[:,idx]]\n",
    "    y_val[:,idx] = [math.log10(i) for i in y_val[:,idx]]\n",
    "\n",
    "# normalization of the physical parameters\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(y_train[:-2])\n",
    "y_train_normalized = scaler.transform(y_train[:-2])\n",
    "y_val_normalized = scaler.transform(y_val[:-2])\n",
    "#y_test_normalized = scaler.transform(y_test[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82224, 128, 43) (82224, 7)\n"
     ]
    }
   ],
   "source": [
    "print(Sy_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make a customized FiLM\n",
    "Mx(t,p) = (1+f(u,p)) Sx(t,p) + g(u,p) \n",
    "1. x(t) is the drum sound corresponding to a stroke location u and shape theta\n",
    "2. Sx(t,p) is the scattering transform of x at the scattering path p=(j1, j2)\n",
    "3. f and g are functions learned by FiLM. They take the location u as an input and return a different number for each scattering path p. The effect of f is multiplicative while the effect of g is additive\n",
    "4. Mx(t,p) is result of feature-wise linear modulation (FiLM). It has the same dimensionality as the scattering transform Sx, but is conditioned on stroke location u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 128, 43) (None, 2)\n",
      "(2, 86)\n"
     ]
    }
   ],
   "source": [
    "input_shape = [(None,128,43),(None,2)]\n",
    "Sc_input_shape, u_input_shape = input_shape\n",
    "print(Sc_input_shape,u_input_shape)\n",
    "height, width = Sc_input_shape[1:] # t, p , 2\n",
    "FiLM_tns = u_input_shape[1]\n",
    "n_feature_maps = width\n",
    "print((FiLM_tns, int(2*n_feature_maps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8095, shape=(5, 1, 4), dtype=int64, numpy=\n",
       "array([[[  4,   5,   6,   7]],\n",
       "\n",
       "       [[ 52,  57,  62,  67]],\n",
       "\n",
       "       [[164, 173, 182, 191]],\n",
       "\n",
       "       [[340, 353, 366, 379]],\n",
       "\n",
       "       [[580, 597, 614, 631]]])>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#kernel (bs,i,2p), u (bs,i), p=2,bs=5, want the output to shape as (bs,2p)\n",
    "F = np.arange(40).reshape(5,2,4)\n",
    "u = np.arange(10).reshape(5,2,1)\n",
    "tf.keras.layers.Dot(axes=(1))([u, F])\n",
    "#(5,1,2),(5,1,2) need to time (bs,t,p) = (5,t,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no activation included FiLM layer - has to add it manually afterwards\n",
    "class FiLM(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FiLM, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape): # input shape = (None,t,p,2)\n",
    "       \n",
    "        Sc_input_shape, u_input_shape = input_shape\n",
    "       # print(Sc_input_shape)\n",
    "        self.height, self.width = Sc_input_shape[1:] # t, p , 2\n",
    "        FiLM_tns = u_input_shape[1]\n",
    "       # print(FiLM_tns,u_input_shape)\n",
    "        self.n_feature_maps = self.width\n",
    "        \n",
    "        #initialize trainable weights, should be 2*2p? f_0 + f_1 u_1 _ f_2 u_2\n",
    "        self.kernel = self.add_weight(name = 'kernel', \n",
    "                                      shape = (FiLM_tns, int(2*self.n_feature_maps)),\n",
    "                                      initializer = 'normal', trainable = True) \n",
    "        \n",
    "        #assert(int(2 * self.n_feature_maps)==FiLM_tns_shape[1]) #film tensor size need to be len(u) by 2p \n",
    "        super(FiLM, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        #assert isinstance(x, list)\n",
    "        conv_output, FiLM_tns = x # x = [Sx,u]; [t by p, length-2]\n",
    "        #FiLM.append(0) # to include the bias term\n",
    "        #(bs,2) dot (bs,2,2p) = (bs,2p)\n",
    "        FiLM_tns = K.dot(FiLM_tns,self.kernel) # u -> [f(u),g(u)] now it's (bs,1,2p)\n",
    "        print(\"before\",FiLM_tns.shape)\n",
    "        #put [f(u),g(u)] in the fourth dimension\n",
    "        FiLM_tns = K.expand_dims(FiLM_tns, axis=[1]) \n",
    "        #FiLM_tns = K.expand_dims(FiLM_tns, axis=[1]) #make it into [1, 1, 1,2p]\n",
    "        FiLM_tns = K.tile(FiLM_tns, [1, self.height, 1]) #[1,Sx.shape[0],Sx.shape[1],2p]\n",
    "        print(FiLM_tns.shape)\n",
    "        #extract f(u) and g(u)\n",
    "        gammas = FiLM_tns[:, :, :self.n_feature_maps] \n",
    "        betas = FiLM_tns[:, :, self.n_feature_maps:]\n",
    "        \n",
    "        # Apply affine transformation\n",
    "        return (1 + gammas) * conv_output + betas\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #assert isinstance(input_shape, list)\n",
    "        #return (input_shape[1],input_shape[2],self.n_feature_maps) # \n",
    "        return input_shape[:-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 128, 43)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 128, 43)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FiLM_layer (FiLM)               [(None, 128, 43)]    172         input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 43)      172         FiLM_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 128, 16)      5520        batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 5,864\n",
      "Trainable params: 5,778\n",
      "Non-trainable params: 86\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Sc_input_shape = (Sy_train.shape[1],Sy_train.shape[2]) \n",
    "u_input_shape = (2,)\n",
    "\n",
    "Sc_input = keras.layers.Input(shape=Sc_input_shape)\n",
    "u_input = keras.layers.Input(shape=u_input_shape)\n",
    "\n",
    "#input_model = keras.layers.Concatenate()([Sc_input, u_input])\n",
    "x = FiLM(input_shape = [Sc_input_shape,u_input_shape],name='FiLM_layer',dynamic=True)([Sc_input,u_input])\n",
    "print(x[0].shape)\n",
    "#x = Dense(20)(x)\n",
    "#model.add(BatchNormalization(input_shape=Sc_input_shape))\n",
    "x = BatchNormalization()(x[0])\n",
    "x = Conv1D(filters=16,\n",
    "        kernel_size=(8,), padding=\"same\",name='conv1')(x)\n",
    "model = Model(inputs=[Sc_input, u_input], outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_adjustable(J,Q,order,k_size,nchan_out,activation):\n",
    "    N = 2**15\n",
    "    y = np.random.rand(N)\n",
    "    scattering = Scattering1D(J = J,shape=(N,), Q = Q, max_order=order) \n",
    "    Sy = np.array(scattering(torch.Tensor(y))).T\n",
    "    nchan_in = 1       # number of input channels.  1 since it is BW\n",
    "\n",
    "    #initialize input sizes\n",
    "    Sc_input_shape = Sy.shape\n",
    "    u_input_shape = (2,)\n",
    "\n",
    "    \n",
    "    input_shape = [Sc_input_shape,u_input_shape]#Sy.shape\n",
    "    kernel_size = (k_size,)\n",
    "    \n",
    "    \n",
    "    K.clear_session()\n",
    "    #define input\n",
    "    Sc_input = keras.layers.Input(shape=Sc_input_shape)\n",
    "    u_input = keras.layers.Input(shape=u_input_shape)\n",
    "    \n",
    "    x = FiLM(input_shape = [Sc_input_shape,u_input_shape],name='FiLM_layer',dynamic=True)([Sc_input,u_input])\n",
    "    \n",
    "    #1 conv layer +  1 batch normalization + nonlinear activation + pooling\n",
    "    x = BatchNormalization()(x[0])\n",
    "    \n",
    "    x = Conv1D(filters=nchan_out,\n",
    "        kernel_size=kernel_size, padding=\"same\",name='conv1')(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(\"relu\"))\n",
    "\n",
    "    if x[0].shape[1]>=4:\n",
    "        pool = 4\n",
    "    elif x[0].shape[1]==2:\n",
    "        pool = 2\n",
    "\n",
    "    #model.add(AveragePooling1D(pool_size=(pool,)))\n",
    "    x = AveragePooling1D(pool_size=(pool,))(x)\n",
    "\n",
    "    for i in range(3):\n",
    "        \n",
    "        #model.add(Conv1D(filters=nchan_out,\n",
    "        #             kernel_size=kernel_size, padding=\"same\" ))\n",
    "        \n",
    "        x = Conv1D(filters=nchan_out,\n",
    "                     kernel_size=kernel_size, padding=\"same\" )(x)\n",
    "        \n",
    "        x = BatchNormalization()(x)\n",
    "        #model.add(BatchNormalization())\n",
    "        x = Activation(\"relu\")(x)\n",
    "        #print(x)\n",
    "        #model.add(Activation(\"relu\"))\n",
    "        #print('before pool',model.layers[-1].output_shape)\n",
    "        if x.shape[1] >= 4:\n",
    "            x = AveragePooling1D(pool_size=(4,))(x)\n",
    "            #model.add(AveragePooling1D(pool_size=(4,)))\n",
    "        elif x.shape[1] == 2:\n",
    "            x = AveragePooling1D(pool_size=(2,))(x)\n",
    "            #model.add(AveragePooling1D(pool_size=(2,)))\n",
    "        #print(model.layers[-1].output_shape)\n",
    "\n",
    "    #model.add(BatchNormalization())\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "    #model.add(Flatten())\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    #model.add(Dense(64, activation='relu'))\n",
    "    x = BatchNormalization()(x)\n",
    "    #model.add(BatchNormalization())\n",
    "    #what activation should be chosen for last layer, for regression problem? should be a linear function\n",
    "    x = Dense(5, activation=activation)(x)\n",
    "   # model.add(Dense(5, activation=activation)) #output layer that corresponds to the 5 physical parameters.\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    model = Model(inputs=[Sc_input, u_input], outputs=x)\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = create_model_adjustable(8,1,2,k_size=8,nchan_out=16,activation=\"linear\")\n",
    "#model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82224, 7)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 82224 samples\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n",
      "before (64, 86)\n",
      "(64, 128, 86)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-298-6df83713039c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;31m#validation_data = (Sy_val_log2[:,-round(Sy_val.shape[1] * 1):,:],y_val),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 use_multiprocessing=False)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mdistributed_function\u001b[0;34m(input_iterator)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     outputs = strategy.experimental_run_v2(\n\u001b[0;32m---> 73\u001b[0;31m         per_replica_function, args=(model, x, y, sample_weights))\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Out of PerReplica outputs reduce or pick values to return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     all_outputs = dist_utils.unwrap_output_dict(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mexperimental_run_v2\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    758\u001b[0m       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\n\u001b[1;32m    759\u001b[0m                                 convert_by_default=False)\n\u001b[0;32m--> 760\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1787\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[0;32m-> 2132\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2134\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m           \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[1;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    594\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m           data_format=data_format),\n\u001b[0m\u001b[1;32m    597\u001b[0m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[1;32m    598\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Sy_train = np.asarray(Sy_train).astype(np.float32)\n",
    "y_train_u = np.asarray(y_train[:,-2:]).astype(np.float32)\n",
    "y_train_theta = np.asarray(y_train[:,:-2]).astype(np.float32)\n",
    "hist = model2.fit([Sy_train,y_train_u],\n",
    "                y_train_theta,\n",
    "                epochs=1,\n",
    "                verbose=2,\n",
    "                batch_size=64,\n",
    "                #validation_data = (Sy_val_log2[:,-round(Sy_val.shape[1] * 1):,:],y_val),\n",
    "                use_multiprocessing=False)\n",
    "\n",
    "validation_loss = hist.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial_dir = \"../output/10trials_\"+str(index)+\"/tests\"+str(trial)+\"/\"\n",
    "\n",
    "os.makedirs(trial_dir, exist_ok=True)\n",
    "best_validation_loss = np.inf\n",
    "zoom_factor = 1\n",
    "n = Sy_train.shape[0]\n",
    "shape_time = round(Sy_train.shape[1] * zoom_factor)\n",
    "steps_per_epoch = 50\n",
    "bs = 64\n",
    "m = bs*steps_per_epoch\n",
    "idx = np.arange(0,n,1)\n",
    "val_loss=[]\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "model_adjustable = create_model_adjustable(J=J,Q=Q,order=order,k_size=8,nchan_out=16,activation='linear')\n",
    "save_log = os.path.join(trial_dir,pickle_name+\"_score.pkl\")\n",
    "#model_adjustable.summary()\n",
    "print('Start fitting the model...')\n",
    "for epoch in range(30):\n",
    "\tnp.random.shuffle(idx)\n",
    "\tSy_temp = Sy_train_log2[idx[:m],:shape_time,:]\n",
    "\ty_temp = y_train_normalized[idx[:m],:]\n",
    "\n",
    "\thist = model_adjustable.fit(Sy_temp,\n",
    "\t\t\t\ty_temp,\n",
    "\t\t\t\tepochs=1,\n",
    "\t\t\t\tverbose=2,\n",
    "\t\t\t\tbatch_size=bs,\n",
    "\t\t\t\tvalidation_data = (Sy_val_log2[:,-shape_time:,:],y_val_normalized),\n",
    "\t\t\t\tuse_multiprocessing=False)\n",
    "\n",
    "\tvalidation_loss = hist.history['val_loss'][0]\n",
    "\n",
    "\ttest_loss.append(model_adjustable.evaluate(Sy_test_log2,y_test_normalized)[0])\n",
    "\tval_loss.append(validation_loss)\n",
    "\ttrain_loss.append(hist.history['loss'][0])\n",
    "\n",
    "\tif validation_loss < best_validation_loss:\n",
    "\t\tbest_validation_loss = validation_loss\n",
    "\t\t#epoch_str = \"epoch-\" + str(epoch).zfill(3)\n",
    "\t\tepoch_network_path = os.path.join(\n",
    "\t\t   trial_dir, \"_\".join([ \"J-\" + str(J).zfill(2), \"Q-\" + str(Q).zfill(2), \"order\" + str(order)]) + \".h5\")\n",
    "\t\tmodel_adjustable.save(epoch_network_path)\n",
    "\n",
    "\n",
    "\n",
    "\t\twith open(save_log, 'wb') as filehandle:\n",
    "\t\t    # store the data as binary data stream\n",
    "\t\t    pickle.dump([val_loss[-1],train_loss[-1],test_loss[-1]], filehandle)\n",
    "\n",
    "print('Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2 ** 15\n",
    "J=6\n",
    "order=1\n",
    "Q = 1\n",
    "scattering = Scattering1D(J=J, shape=(N,), Q=Q, max_order=order)\n",
    "\n",
    "\n",
    "\n",
    "waveform,_ = librosa.load(\"./3643_sound.wav\")\n",
    "torch_waveform = torch.Tensor(waveform)\n",
    "Sx = np.array(scattering(torch_waveform).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./diffshapes_param.csv\")\n",
    "sample_ids = df.values[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.values[:, 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
