{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FiLM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "from tensorflow import keras\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv1D, AveragePooling1D, Conv2D, MaxPooling2D,ReLU\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import load_model #save and load models\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint\n",
    "import IPython.display as ipd\n",
    "from kymatio import Scattering1D\n",
    "import hitdifferentparts\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pescador\n",
    "import random\n",
    "import os\n",
    "import librosa\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download an example file from hpc\n",
    "pkl_trainpath = \"./scattering_fold-train_J-08_Q-01_order2.pkl\"\n",
    "pkl_valpath = \"./scattering_fold-val_J-08_Q-01_order2.pkl\"\n",
    "#pkl_testpath = \"./scattering_fold-test_J-08_Q-02_order2.pkl\"\n",
    "pkl_train = open(pkl_trainpath, 'rb')\n",
    "#pkl_test = open(pkl_testpath, 'rb')\n",
    "pkl_val = open(pkl_valpath, 'rb')\n",
    "\n",
    "Sy_train,y_train = pickle.load(pkl_train) \n",
    "#Sy_test,y_test = pickle.load(pkl_test) \n",
    "Sy_val,y_val = pickle.load(pkl_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps =1e-11\n",
    "Sy_train_log2 = np.log1p(((Sy_train>0)*Sy_train)/eps)\n",
    "Sy_val_log2 = np.log1p(((Sy_val>0)*Sy_val)/eps)\n",
    "#Sy_test_log2 = np.log1p((Sy_test>0)*Sy_test/eps)\n",
    "\n",
    "#log scale p and D\n",
    "for idx in range(2,4):\n",
    "    y_train[:,idx] = [math.log10(i) for i in y_train[:,idx]]\n",
    "    #y_test[:,idx] = [math.log10(i) for i in y_test[:,idx]]\n",
    "    y_val[:,idx] = [math.log10(i) for i in y_val[:,idx]]\n",
    "\n",
    "# normalization of the physical parameters\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(y_train[:-2])\n",
    "y_train_normalized = scaler.transform(y_train[:-2])\n",
    "y_val_normalized = scaler.transform(y_val[:-2])\n",
    "#y_test_normalized = scaler.transform(y_test[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82224, 128, 43) (82224, 7)\n"
     ]
    }
   ],
   "source": [
    "print(Sy_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make a customized FiLM\n",
    "Mx(t,p) = (1+f(u,p)) Sx(t,p) + g(u,p) \n",
    "1. x(t) is the drum sound corresponding to a stroke location u and shape theta\n",
    "2. Sx(t,p) is the scattering transform of x at the scattering path p=(j1, j2)\n",
    "3. f and g are functions learned by FiLM. They take the location u as an input and return a different number for each scattering path p. The effect of f is multiplicative while the effect of g is additive\n",
    "4. Mx(t,p) is result of feature-wise linear modulation (FiLM). It has the same dimensionality as the scattering transform Sx, but is conditioned on stroke location u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 128, 43) (None, 2)\n",
      "(2, 86)\n"
     ]
    }
   ],
   "source": [
    "input_shape = [(None,128,43),(None,2)]\n",
    "Sc_input_shape, u_input_shape = input_shape\n",
    "print(Sc_input_shape,u_input_shape)\n",
    "height, width = Sc_input_shape[1:] # t, p , 2\n",
    "FiLM_tns = u_input_shape[1]\n",
    "n_feature_maps = width\n",
    "print((FiLM_tns, int(2*n_feature_maps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no activation included FiLM layer - has to add it manually afterwards\n",
    "class FiLM(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FiLM, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape): # input shape = (None,t,p,2)\n",
    "       \n",
    "        Sc_input_shape, u_input_shape = input_shape\n",
    "       # print(Sc_input_shape)\n",
    "        self.height, self.width = Sc_input_shape[1:] # t, p , 2\n",
    "        FiLM_tns = u_input_shape[1]\n",
    "       # print(FiLM_tns,u_input_shape)\n",
    "        self.n_feature_maps = self.width\n",
    "        \n",
    "        #initialize trainable weights, should be 2*2p? f_0 + f_1 u_1 _ f_2 u_2\n",
    "        self.kernel = self.add_weight(name = 'kernel', \n",
    "                                      shape = (FiLM_tns, int(2*self.n_feature_maps)),\n",
    "                                      initializer = 'normal', trainable = True) \n",
    "        \n",
    "        #assert(int(2 * self.n_feature_maps)==FiLM_tns_shape[1]) #film tensor size need to be len(u) by 2p \n",
    "        super(FiLM, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        #assert isinstance(x, list)\n",
    "        conv_output, FiLM_tns = x # x = [Sx,u]; [t by p, length-2]\n",
    "        #FiLM.append(0) # to include the bias term\n",
    "        FiLM_tns = K.dot(FilM_tns,self.kernel) # u -> [f(u),g(u)]\n",
    "\n",
    "        #put [f(u),g(u)] in the fourth dimension\n",
    "        FiLM_tns = K.expand_dims(FiLM_tns, axis=[1]) \n",
    "        FiLM_tns = K.expand_dims(FiLM_tns, axis=[1]) #make it into [1, 1, 1,2p]\n",
    "        FiLM_tns = K.tile(FiLM_tns, [1, self.height, self.width, 1]) #[1,Sx.shape[0],Sx.shape[1],2p]\n",
    "        \n",
    "        #extract f(u) and g(u)\n",
    "        gammas = FiLM_tns[:, :, :, :self.n_feature_maps] \n",
    "        betas = FiLM_tns[:, :, :, self.n_feature_maps:]\n",
    "        \n",
    "        # Apply affine transformation\n",
    "        return (1 + gammas) * conv_output + betas\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #assert isinstance(input_shape, list)\n",
    "        #return (input_shape[1],input_shape[2],self.n_feature_maps) # \n",
    "        return input_shape[:-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"FiLM_layer_1/Placeholder:0\", shape=(None, 128, 43), dtype=float32)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 128, 43)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FiLM_layer (FiLM)               [(None, 128, 43)]    172         input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 43)      172         FiLM_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 128, 16)      5520        batch_normalization_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 5,864\n",
      "Trainable params: 5,778\n",
      "Non-trainable params: 86\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Sc_input_shape = (Sy_train.shape[1],Sy_train.shape[2]) \n",
    "u_input_shape = (2,)\n",
    "\n",
    "Sc_input = keras.layers.Input(shape=Sc_input_shape)\n",
    "u_input = keras.layers.Input(shape=u_input_shape)\n",
    "\n",
    "#input_model = keras.layers.Concatenate()([Sc_input, u_input])\n",
    "x = FiLM(input_shape = [Sc_input_shape,u_input_shape],name='FiLM_layer',dynamic=True)([Sc_input,u_input])\n",
    "print(x[0].shape)\n",
    "#x = Dense(20)(x)\n",
    "#model.add(BatchNormalization(input_shape=Sc_input_shape))\n",
    "x = BatchNormalization()(x[0])\n",
    "x = Conv1D(filters=16,\n",
    "        kernel_size=(8,), padding=\"same\",name='conv1')(x)\n",
    "model = Model(inputs=[Sc_input, u_input], outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_adjustable(J,Q,order,k_size,nchan_out,activation):\n",
    "    N = 2**15\n",
    "    y = np.random.rand(N)\n",
    "    scattering = Scattering1D(J = J,shape=(N,), Q = Q, max_order=order) \n",
    "    Sy = np.array(scattering(torch.Tensor(y))).T\n",
    "    nchan_in = 1       # number of input channels.  1 since it is BW\n",
    "\n",
    "    #initialize input sizes\n",
    "    Sc_input_shape = Sy.shape\n",
    "    u_input_shape = (2,)\n",
    "\n",
    "    \n",
    "    input_shape = [Sc_input_shape,u_input_shape]#Sy.shape\n",
    "    kernel_size = (k_size,)\n",
    "    \n",
    "    \n",
    "    K.clear_session()\n",
    "    #define input\n",
    "    Sc_input = keras.layers.Input(shape=Sc_input_shape)\n",
    "    u_input = keras.layers.Input(shape=u_input_shape)\n",
    "    \n",
    "    x = FiLM(input_shape = [Sc_input_shape,u_input_shape],name='FiLM_layer',dynamic=True)([Sc_input,u_input])\n",
    "    \n",
    "    #1 conv layer +  1 batch normalization + nonlinear activation + pooling\n",
    "    x = BatchNormalization()(x[0])\n",
    "    \n",
    "    x = Conv1D(filters=nchan_out,\n",
    "        kernel_size=kernel_size, padding=\"same\",name='conv1')(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation(\"relu\"))\n",
    "\n",
    "    if x[0].shape[1]>=4:\n",
    "        pool = 4\n",
    "    elif x[0].shape[1]==2:\n",
    "        pool = 2\n",
    "\n",
    "    #model.add(AveragePooling1D(pool_size=(pool,)))\n",
    "    x = AveragePooling1D(pool_size=(pool,))(x)\n",
    "\n",
    "    for i in range(3):\n",
    "        \n",
    "        #model.add(Conv1D(filters=nchan_out,\n",
    "        #             kernel_size=kernel_size, padding=\"same\" ))\n",
    "        \n",
    "        x = Conv1D(filters=nchan_out,\n",
    "                     kernel_size=kernel_size, padding=\"same\" )(x)\n",
    "        \n",
    "        x = BatchNormalization()(x)\n",
    "        #model.add(BatchNormalization())\n",
    "        x = Activation(\"relu\")(x)\n",
    "        #print(x)\n",
    "        #model.add(Activation(\"relu\"))\n",
    "        #print('before pool',model.layers[-1].output_shape)\n",
    "        if x.shape[1] >= 4:\n",
    "            x = AveragePooling1D(pool_size=(4,))(x)\n",
    "            #model.add(AveragePooling1D(pool_size=(4,)))\n",
    "        elif x.shape[1] == 2:\n",
    "            x = AveragePooling1D(pool_size=(2,))(x)\n",
    "            #model.add(AveragePooling1D(pool_size=(2,)))\n",
    "        #print(model.layers[-1].output_shape)\n",
    "\n",
    "    #model.add(BatchNormalization())\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "    #model.add(Flatten())\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    #model.add(Dense(64, activation='relu'))\n",
    "    x = BatchNormalization()(x)\n",
    "    #model.add(BatchNormalization())\n",
    "    #what activation should be chosen for last layer, for regression problem? should be a linear function\n",
    "    x = Dense(5, activation=activation)(x)\n",
    "   # model.add(Dense(5, activation=activation)) #output layer that corresponds to the 5 physical parameters.\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    model = Model(inputs=[Sc_input, u_input], outputs=x)\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 43)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FiLM_layer (FiLM)               [(None, 128, 43)]    172         input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 43)      172         FiLM_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 128, 16)      5520        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 16)      0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d (AveragePooli (None, 32, 16)       0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 32, 16)       2064        average_pooling1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 16)       64          conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 16)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_1 (AveragePoo (None, 8, 16)        0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 8, 16)        2064        average_pooling1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 8, 16)        64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 8, 16)        0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_2 (AveragePoo (None, 2, 16)        0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 2, 16)        2064        average_pooling1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 2, 16)        64          conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 2, 16)        0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_3 (AveragePoo (None, 1, 16)        0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1, 16)        64          average_pooling1d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 16)           0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           1088        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64)           256         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 5)            325         batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 13,981\n",
      "Trainable params: 13,639\n",
      "Non-trainable params: 342\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = create_model_adjustable(8,1,2,k_size=8,nchan_out=16,activation=\"linear\")\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(128, 43), 2]\n"
     ]
    }
   ],
   "source": [
    "Sc_input_shape = (Sy_train.shape[1],Sy_train.shape[2]) \n",
    "u_input_shape = (2)\n",
    "print([Sc_input_shape,u_input_shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-249-2cf509729722>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0;31m#validation_data = (Sy_val_log2[:,-round(Sy_val.shape[1] * 1):,:],y_val),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 use_multiprocessing=False)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    607\u001b[0m   \u001b[0;31m# As a fallback for the data type that does not work with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m   \u001b[0;31m# _standardize_user_data, use the _prepare_model_with_inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m                **kwargs):\n\u001b[1;32m    216\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_numpy_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_numpy_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_numpy_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_numpy_inputs\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_non_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m   \u001b[0;31m# For more complicated structure, we only convert the out most list to tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m   \u001b[0;31m# since dataset will stack the list, but treat elements in the tuple as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 535\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 535\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_non_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;31m# `SparseTensors` can't be converted to `Tensor`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[1;32m   1182\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[1;32m   1183\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[0;32m-> 1184\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1240\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    225\u001b[0m   \"\"\"\n\u001b[1;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 227\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = model2.fit([Sy_train,y_train[:,-2:]],\n",
    "                y_train[:,:-2],\n",
    "                epochs=1,\n",
    "                verbose=2,\n",
    "                batch_size=64,\n",
    "                #validation_data = (Sy_val_log2[:,-round(Sy_val.shape[1] * 1):,:],y_val),\n",
    "                use_multiprocessing=False)\n",
    "\n",
    "validation_loss = hist.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial_dir = \"../output/10trials_\"+str(index)+\"/tests\"+str(trial)+\"/\"\n",
    "\n",
    "os.makedirs(trial_dir, exist_ok=True)\n",
    "best_validation_loss = np.inf\n",
    "zoom_factor = 1\n",
    "n = Sy_train.shape[0]\n",
    "shape_time = round(Sy_train.shape[1] * zoom_factor)\n",
    "steps_per_epoch = 50\n",
    "bs = 64\n",
    "m = bs*steps_per_epoch\n",
    "idx = np.arange(0,n,1)\n",
    "val_loss=[]\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "model_adjustable = create_model_adjustable(J=J,Q=Q,order=order,k_size=8,nchan_out=16,activation='linear')\n",
    "save_log = os.path.join(trial_dir,pickle_name+\"_score.pkl\")\n",
    "#model_adjustable.summary()\n",
    "print('Start fitting the model...')\n",
    "for epoch in range(30):\n",
    "\tnp.random.shuffle(idx)\n",
    "\tSy_temp = Sy_train_log2[idx[:m],:shape_time,:]\n",
    "\ty_temp = y_train_normalized[idx[:m],:]\n",
    "\n",
    "\thist = model_adjustable.fit(Sy_temp,\n",
    "\t\t\t\ty_temp,\n",
    "\t\t\t\tepochs=1,\n",
    "\t\t\t\tverbose=2,\n",
    "\t\t\t\tbatch_size=bs,\n",
    "\t\t\t\tvalidation_data = (Sy_val_log2[:,-shape_time:,:],y_val_normalized),\n",
    "\t\t\t\tuse_multiprocessing=False)\n",
    "\n",
    "\tvalidation_loss = hist.history['val_loss'][0]\n",
    "\n",
    "\ttest_loss.append(model_adjustable.evaluate(Sy_test_log2,y_test_normalized)[0])\n",
    "\tval_loss.append(validation_loss)\n",
    "\ttrain_loss.append(hist.history['loss'][0])\n",
    "\n",
    "\tif validation_loss < best_validation_loss:\n",
    "\t\tbest_validation_loss = validation_loss\n",
    "\t\t#epoch_str = \"epoch-\" + str(epoch).zfill(3)\n",
    "\t\tepoch_network_path = os.path.join(\n",
    "\t\t   trial_dir, \"_\".join([ \"J-\" + str(J).zfill(2), \"Q-\" + str(Q).zfill(2), \"order\" + str(order)]) + \".h5\")\n",
    "\t\tmodel_adjustable.save(epoch_network_path)\n",
    "\n",
    "\n",
    "\n",
    "\t\twith open(save_log, 'wb') as filehandle:\n",
    "\t\t    # store the data as binary data stream\n",
    "\t\t    pickle.dump([val_loss[-1],train_loss[-1],test_loss[-1]], filehandle)\n",
    "\n",
    "print('Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2 ** 15\n",
    "J=6\n",
    "order=1\n",
    "Q = 1\n",
    "scattering = Scattering1D(J=J, shape=(N,), Q=Q, max_order=order)\n",
    "\n",
    "\n",
    "\n",
    "waveform,_ = librosa.load(\"./3643_sound.wav\")\n",
    "torch_waveform = torch.Tensor(waveform)\n",
    "Sx = np.array(scattering(torch_waveform).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./diffshapes_param.csv\")\n",
    "sample_ids = df.values[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.values[:, 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
