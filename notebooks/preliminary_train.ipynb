{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv1D, AveragePooling1D, Conv2D, MaxPooling2D,ReLU\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import load_model #save and load models\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint\n",
    "import IPython.display as ipd\n",
    "from kymatio import Scattering1D\n",
    "import hitdifferentparts\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pescador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify if using GPU\n",
    "\n",
    "#tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scattering order one\n",
    "def getsc_new(y,J,Q_num,order):\n",
    "    \"\"\"\n",
    "    this function outputs scattering transform of a time-domain signal.\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    scattering = Scattering1D(J = J,shape=(N,), Q = Q_num, max_order=order)\n",
    "    Sy = scattering(torch.Tensor(y))\n",
    "    return Sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./train_param.csv\")\n",
    "df_test = pd.read_csv(\"./test_param.csv\")\n",
    "df_val = pd.read_csv(\"./val_param.csv\")\n",
    "df_full = pd.read_csv(\"./diffshapes_param.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of the physical parameters\n",
    "params = df_train.values[:,1:-1]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(params)\n",
    "train_params_normalized = scaler.transform(params)\n",
    "test_params_normalized = scaler.transform(df_test.values[:,1:-1])\n",
    "val_params_normalized = scaler.transform(df_val.values[:,1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82224"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_sampler(df,params_normalized,idx,path_to_folder,J,Q,order):\n",
    "    \"\"\"\n",
    "    output a {input, ground truth} pair for the designated audio sample\n",
    "    \"\"\"\n",
    "    i=idx\n",
    "    y=np.array(params_normalized[i,:]).reshape((5,)) #df.values[i,1:-1]\n",
    "    path_to_audio = os.path.join(path_to_folder,str(df.values[i,0])+\"_sound.wav\") \n",
    "    x,fs=librosa.load(path_to_audio)\n",
    "    Sy = getsc_new(x,J,Q,order)\n",
    "    m,n = Sy.shape\n",
    "    Sy2 = np.array(Sy).reshape((n,m))\n",
    "    \n",
    "    while True:\n",
    "        yield {'input': Sy2,'y': y}\n",
    "\n",
    "        \n",
    "def data_generator(df, params_normalized, path_to_folder, J, Q, order, batch_size, idx, active_streamers,\n",
    "                        rate, random_state=12345678):\n",
    "    \"\"\"\n",
    "    use streamers to output a batch of {input groundtruth} pairs. \n",
    "    \"\"\"\n",
    "    seeds = []\n",
    "    for i in idx:\n",
    "        streamer = pescador.Streamer(feature_sampler, df, params_normalized, i,path_to_folder,J,Q,order)\n",
    "        seeds.append(streamer)\n",
    "\n",
    "    # Randomly shuffle the seeds\n",
    "    random.shuffle(seeds)\n",
    "\n",
    "    mux = pescador.StochasticMux(seeds, active_streamers, rate=rate, random_state=random_state)\n",
    "   \n",
    "    if batch_size == 1:\n",
    "        return mux\n",
    "    else:\n",
    "        return pescador.maps.buffer_stream(mux, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first run with small number of training\n",
    "epochs=12\n",
    "batch_size=32\n",
    "random_state=12345678\n",
    "active_streamers=64\n",
    "path_to_train = \"/scratch/hh2263/drum_data/train/\"\n",
    "path_to_test = \"/scratch/hh2263/drum_data/test/\"\n",
    "J = 8\n",
    "Q = 1\n",
    "order = 2 # remember to go to order 2 eventually\n",
    "train_idx = np.arange(0,params.shape[0],1)#np.arange(0,1000,1) #df_train.values[:1000,0]\n",
    "#test_idx = np.arange(0,300,1) #df_test.values[:300,0]\n",
    "train_batches=data_generator(df_train,train_params_normalized, path_to_train,J, Q, order, batch_size, train_idx,active_streamers,rate=64,random_state=random_state)\n",
    "#test_batches=data_generator(df_test,test_params_normalized, path_to_test,J, Q, order, batch_size, test_idx,active_streamers,rate=64,random_state=random_state)\n",
    "steps_per_epoch = 10 #len(train_idx) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = random.choice(os.listdir(path_to_train))\n",
    "rand_audio = os.path.join(path_to_train,fname)\n",
    "y,sr = librosa.load(rand_audio)\n",
    "Sy = getsc_new(torch.Tensor(y),J,Q,order).T\n",
    "nrow, ncol = Sy.shape \n",
    "naudio = batch_size         # number of images in batch\n",
    "nchan_in = 1       # number of input channels.  1 since it is BW\n",
    "#input_shape = (nrow,ncol,nchan_in)  # input shape of 1 image\n",
    "input_shape = Sy.shape\n",
    "batch_shape = (naudio,nrow,ncol,nchan_in)  # shape of image batch\n",
    "#x = Sy.reshape(batch_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 43]) (32, 128, 43, 1)\n"
     ]
    }
   ],
   "source": [
    "print(input_shape,batch_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hh2263/miniconda3/envs/drum-1.15/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv1D)               (None, 128, 16)           5520      \n",
      "_________________________________________________________________\n",
      "average_pooling1d (AveragePo (None, 32, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv1D)               (None, 32, 16)            2064      \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv1D)               (None, 8, 16)             2064      \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 2, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv1D)               (None, 2, 16)             2064      \n",
      "_________________________________________________________________\n",
      "average_pooling1d_3 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1, 16)             64        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 13,445\n",
      "Trainable params: 13,285\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "kernel_size = (8,)\n",
    "nchan_out = 16\n",
    "\n",
    "K.clear_session()\n",
    "model=Sequential()\n",
    "model.add(Conv1D(input_shape=input_shape, filters=nchan_out,\n",
    "                 kernel_size=kernel_size,activation= \"relu\", padding=\"same\",name='conv1'))\n",
    "model.add(AveragePooling1D(pool_size=(4,)))\n",
    "model.add(Conv1D(filters=16,\n",
    "                 kernel_size=kernel_size,activation= \"relu\", padding=\"same\",name='conv2' ))\n",
    "model.add(AveragePooling1D(pool_size=(4,)))\n",
    "model.add(Conv1D(filters=16,\n",
    "                 kernel_size=kernel_size,activation= \"relu\", padding=\"same\",name='conv3' ))\n",
    "model.add(AveragePooling1D(pool_size=(4,)))\n",
    "model.add(Conv1D(filters=16,\n",
    "                 kernel_size=kernel_size,activation= \"relu\", padding=\"same\",name='conv4' ))\n",
    "model.add(AveragePooling1D(pool_size=(2,)))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "#what activation should be chosen for last layer, for regression problem? should be a linear function\n",
    "model.add(Dense(5, activation='linear')) #output layer that corresponds to the 5 physical parameters.\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "output_dir = \"../output/\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "model_filepath = os.path.join(output_dir, 'model.h5')\n",
    "log_filepath = os.path.join(output_dir, 'train_log.csv')\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(EarlyStopping(patience=10))\n",
    "callbacks.append(ModelCheckpoint(model_filepath, save_best_only=True))\n",
    "callbacks.append(CSVLogger(log_filepath))\n",
    "\n",
    "print(\"Fitting model.\")\n",
    "sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = pescador.maps.keras_tuples(train_batches, 'input', 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7776, 128, 43) (7776, 5)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pkl_path = '/scratch/hh2263/drum_data/val/J_8_Q_1_order_2.pkl'\n",
    "pkl_file = open(pkl_path, 'rb')\n",
    "Sy_val,y_val = pickle.load(pkl_file) \n",
    "\n",
    "Sy_val = Sy_val.reshape((Sy_val.shape[2],Sy_val.shape[0],Sy_val.shape[1]))\n",
    "#Sy_val = torch.Tensor(Sy_val)\n",
    "y_val = y_val.astype('float32')\n",
    "#y_val = torch.Tensor(y_val.astype('float32'))\n",
    "print(Sy_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.6306809e+03 9.4744407e-02 1.5361507e-03 2.7646576e-03 3.1666741e-01] [2.5150061e-01 2.9220518e-01 7.1812602e-04 2.6654196e-04 3.1599283e-01]\n"
     ]
    }
   ],
   "source": [
    "y_val_normalized = scaler.transform(y_val)\n",
    "print(y_val[1,:],y_val_normalized[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 48s 5s/step - loss: 0.1732 - acc: 0.4437\n",
      "done fitting\n",
      "7776/7776 [==============================] - 1s 109us/sample - loss: 0.1322 - acc: 0.3329\n",
      "0.13220949009551433 0.33294752\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.1239 - acc: 0.6844\n",
      "done fitting\n",
      "7776/7776 [==============================] - 1s 104us/sample - loss: 0.0975 - acc: 0.3329\n",
      "0.09747603526453913 0.33294752\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.0863 - acc: 0.7437\n",
      "done fitting\n",
      "7776/7776 [==============================] - 1s 112us/sample - loss: 0.0672 - acc: 0.3331\n",
      "0.06718635030949312 0.33307612\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.0592 - acc: 0.7500\n",
      "done fitting\n",
      "7776/7776 [==============================] - 1s 110us/sample - loss: 0.0437 - acc: 0.3331\n",
      "0.04369594477914243 0.33307612\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.0409 - acc: 0.7563\n",
      "done fitting\n",
      "7776/7776 [==============================] - 1s 116us/sample - loss: 0.0288 - acc: 0.3331\n",
      "0.028772203964583666 0.33307612\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.0316 - acc: 0.7563\n",
      "done fitting\n",
      "7776/7776 [==============================] - 1s 108us/sample - loss: 0.0221 - acc: 0.3331\n",
      "0.022054807666244576 0.33307612\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.0286 - acc: 0.7531\n",
      "done fitting\n",
      "7776/7776 [==============================] - 1s 104us/sample - loss: 0.0201 - acc: 0.3331\n",
      "0.020106098953818465 0.33307612\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.0280 - acc: 0.7625\n",
      "done fitting\n",
      "7776/7776 [==============================] - 1s 106us/sample - loss: 0.0197 - acc: 0.3331\n",
      "0.01974065073692995 0.33307612\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.0275 - acc: 0.7563\n",
      "done fitting\n",
      "7776/7776 [==============================] - 1s 120us/sample - loss: 0.0198 - acc: 0.3331\n",
      "0.01982364331567545 0.33307612\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.0271 - acc: 0.7625\n",
      "done fitting\n",
      "7776/7776 [==============================] - 1s 105us/sample - loss: 0.0200 - acc: 0.3331\n",
      "0.019981272528216305 0.33307612\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.0266 - acc: 0.7531\n",
      "done fitting\n",
      "7776/7776 [==============================] - 1s 125us/sample - loss: 0.0201 - acc: 0.3331\n",
      "0.020095710611974998 0.33307612\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.0260 - acc: 0.7500\n",
      "done fitting\n",
      "7776/7776 [==============================] - 1s 138us/sample - loss: 0.0202 - acc: 0.3331\n",
      "0.0201544643996797 0.33307612\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.fit(train_gen,steps_per_epoch=10,epochs=1,use_multiprocessing=True)\n",
    "    print('done fitting')\n",
    "    loss,accuracy = model.evaluate(Sy_val,y_val_normalized)\n",
    "    print(loss,accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps, validate for 1024 steps\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 495s 495s/step - loss: 0.1938 - accuracy: 0.3438 - val_loss: 0.2221 - val_accuracy: 0.3015\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 218s 218s/step - loss: 0.2108 - accuracy: 0.1875 - val_loss: 0.2186 - val_accuracy: 0.2869\n"
     ]
    }
   ],
   "source": [
    "#preliminary test\n",
    "hist = model.fit(\n",
    "        pescador.maps.keras_tuples(train_batches, 'input', 'y'),\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=2,\n",
    "        validation_data=pescador.maps.keras_tuples(test_batches, 'input', 'y'),\n",
    "        validation_steps=1024,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model(model_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
